{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366bf9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DenseDiffusion/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import diffusers\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion_3.pipeline_dense_stable_diffusion_3 import DenseStableDiffusion3Pipeline\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "import transformers\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    T5EncoderModel,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "token = \"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\"\n",
    "device= \"cuda\"\n",
    "\n",
    "with open('./dataset/testset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "layout_img_root = './dataset/testset_layout/'\n",
    "# with open('./dataset/valset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)\n",
    "# layout_img_root = './dataset/valset_layout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da96e1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  11%|█         | 1/9 [00:03<00:25,  3.19s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:13<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "pipe = DenseStableDiffusion3Pipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    cache_dir='./models/diffusers/',\n",
    "    # text_encoder_3=None,\n",
    "    # tokenizer_3=None,\n",
    "    torch_dtype=torch.bfloat16\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d79268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13629/754738756.py:4: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  sp_sz = pipe.transformer.sample_size\n"
     ]
    }
   ],
   "source": [
    "STEPS=10\n",
    "pipe.scheduler.set_timesteps(STEPS)\n",
    "timesteps = pipe.scheduler.timesteps\n",
    "sp_sz = pipe.transformer.sample_size\n",
    "bsz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8e92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 17\n",
    "layout_img_path = layout_img_root+str(idx)+'.png'\n",
    "prompts = [dataset[idx]['textual_condition']] + dataset[idx]['segment_descriptions']\n",
    "\n",
    "########### tokenizer 1 and text encoder 1\n",
    "text_input = pipe.tokenizer(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                            max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input = pipe.tokenizer([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input['length'][i] - 2\n",
    "    widx = text_input['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            break\n",
    "\n",
    "############ tokenizer 2 and text encoder 2\n",
    "text_input_2 = pipe.tokenizer_2(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                               max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings_2 = pipe.text_encoder_2(text_input_2.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input_2 = pipe.tokenizer_2([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings_2 = pipe.text_encoder_2(uncond_input_2.input_ids.to(device))[0]\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input_2['length'][i] - 2\n",
    "    widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            break\n",
    "        \n",
    "############ tokenizer 3 and text encoder 3\n",
    "text_input_3 = pipe.tokenizer_3(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                               max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings_3 = pipe.text_encoder_3(text_input_3.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input_3 = pipe.tokenizer_3([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings_3 = pipe.text_encoder_3(uncond_input_3.input_ids.to(device))[0]\n",
    "\n",
    "############\n",
    "layout_img_ = np.asarray(Image.open(layout_img_path).resize([sp_sz*8,sp_sz*8]))[:,:,:3]\n",
    "unique, counts = np.unique(np.reshape(layout_img_,(-1,3)), axis=0, return_counts=True)\n",
    "sorted_idx = np.argsort(-counts)\n",
    "\n",
    "layouts_ = []\n",
    "\n",
    "for i in range(len(prompts)-1):\n",
    "    if (unique[sorted_idx[i]] == [0, 0, 0]).all() or (unique[sorted_idx[i]] == [255, 255, 255]).all():\n",
    "        layouts_ = [((layout_img_ == unique[sorted_idx[i]]).sum(-1)==3).astype(np.uint8)] + layouts_\n",
    "    else:\n",
    "        layouts_.append(((layout_img_ == unique[sorted_idx[i]]).sum(-1)==3).astype(np.uint8))\n",
    "        \n",
    "layouts = [torch.FloatTensor(l).unsqueeze(0).unsqueeze(0).cuda() for l in layouts_]\n",
    "layouts = F.interpolate(torch.cat(layouts),(sp_sz,sp_sz),mode='nearest')\n",
    "\n",
    "layouts_cross = [torch.FloatTensor(l).unsqueeze(0).unsqueeze(0).cuda() for l in layouts_]\n",
    "layouts_cross = F.interpolate(torch.cat(layouts_cross),(sp_sz,sp_sz),mode='nearest')\n",
    "############\n",
    "# print('\\n'.join(prompts))\n",
    "# Image.fromarray(np.concatenate([255*_.squeeze().cpu().numpy() for _ in layouts], 1).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df45a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "from diffusers.models.attention import Attention\n",
    "import math\n",
    "\n",
    "class JointAttnProcessor_mod:\n",
    "    \"\"\"\n",
    "    Attention processor for SD3-like self-attention projections.\n",
    "    This processor handles both self-attention and cross-attention mechanisms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,  # Attention module\n",
    "        hidden_states: torch.FloatTensor,  # Input tensor [batch_size, seq_len, hidden_dim]\n",
    "        encoder_hidden_states: torch.FloatTensor = None,  # Optional context tensor [batch_size, context_len, hidden_dim]\n",
    "        attention_mask = None,  # Optional mask tensor [batch_size, seq_len]\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # Store residual for skip connection\n",
    "        residual = hidden_states  # [batch_size, seq_len, hidden_dim]\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        # Project input into query, key, value spaces\n",
    "        # Each projection: [batch_size, seq_len, hidden_dim]\n",
    "        query = attn.to_q(hidden_states)\n",
    "        key = attn.to_k(hidden_states)\n",
    "        value = attn.to_v(hidden_states)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        inner_dim = key.shape[-1]  # Total dimension across all heads\n",
    "        head_dim = inner_dim // attn.heads  # Dimension per attention head\n",
    "\n",
    "        # Reshape tensors to [batch_size, n_heads, seq_len, head_dim]\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply normalization if specified\n",
    "        if attn.norm_q is not None:\n",
    "            query = attn.norm_q(query)\n",
    "        if attn.norm_k is not None:\n",
    "            key = attn.norm_k(key)\n",
    "        \n",
    "\n",
    "        # Handle cross-attention if encoder_hidden_states is provided\n",
    "        if encoder_hidden_states is not None:\n",
    "            # Project encoder states to query, key, value\n",
    "            # [batch_size, context_len, hidden_dim] -> [batch_size, n_heads, context_len, head_dim]\n",
    "            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)\n",
    "            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n",
    "            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n",
    "\n",
    "            # Reshape encoder projections\n",
    "            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "            # Apply normalization to encoder projections if specified\n",
    "            if attn.norm_added_q is not None:\n",
    "                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)\n",
    "            if attn.norm_added_k is not None:\n",
    "                encoder_hidden_states_key_proj = attn.norm_added_k(encoder_hidden_states_key_proj)\n",
    "\n",
    "            # Concatenate self and cross attention tensors\n",
    "            # [batch_size, n_heads, seq_len + context_len, head_dim]\n",
    "            query = torch.cat([query, encoder_hidden_states_query_proj], dim=2)\n",
    "            key = torch.cat([key, encoder_hidden_states_key_proj], dim=2)\n",
    "            value = torch.cat([value, encoder_hidden_states_value_proj], dim=2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        L, S = query.size(-2), key.size(-2)  # L: target sequence length, S: source sequence length\n",
    "        scale_factor = 1 / math.sqrt(query.size(-1))  # Scaling factor for numerical stability\n",
    "        attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dtype == torch.bool:\n",
    "                attn_bias.masked_fill_(attention_mask.logical_not(), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_bias += attention_mask\n",
    "\n",
    "        # Compute attention weights and apply them to values\n",
    "        # [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        \n",
    "        ################################################# regulation for self attention and self attention\n",
    "        sa_ = True if encoder_hidden_states is None else False\n",
    "        global COUNT\n",
    "        if COUNT/37 < STEPS * reg_part:\n",
    "            treg = torch.pow(timesteps[COUNT//37]/1000, 5)\n",
    "            if sa_:\n",
    "                # self attention regulation\n",
    "                min_value = attn_weight[int(attn_weight.size(0)/2):].min(-1)[0].unsqueeze(-1)\n",
    "                max_value = attn_weight[int(attn_weight.size(0)/2):].max(-1)[0].unsqueeze(-1)  \n",
    "                \n",
    "                mask = sreg_maps[attn_weight.size(2)].repeat(attn.heads,1,1)\n",
    "                \n",
    "                size_reg = reg_sizes[attn_weight.size(2)].repeat(attn.heads,1,1)\n",
    "                \n",
    "                \n",
    "                # Apply positive and negative regulation for self-attention\n",
    "                attn_weight[int(attn_weight.size(0)/2):] += (mask>0)*size_reg*sreg*treg*(max_value-attn_weight[int(attn_weight.size(0)/2):])\n",
    "                attn_weight[int(attn_weight.size(0)/2):] -= ~(mask>0)*size_reg*sreg*treg*(attn_weight[int(attn_weight.size(0)/2):]-min_value)\n",
    "                \n",
    "                attn_weight += attn_bias\n",
    "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "                attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "            else:\n",
    "                # cross attention regulation\n",
    "                \n",
    "                # The upper-left and bottom-right parts of the matrix represent self-attention, while the upper-right and bottom-left parts represent cross-attention.\n",
    "                # Here we only apply regulation to the cross-attention part.\n",
    "                image_length = query.shape[2] - encoder_hidden_states_query_proj.shape[2]\n",
    "                attn_weight_cross_part = attn_weight[:,:,:image_length,image_length:] # 24, 4096, 333\n",
    "\n",
    "                min_value = attn_weight_cross_part[int(attn_weight_cross_part.size(0)/2):].min(-1)[0].unsqueeze(-1) # 4, 24, 4096, 1\n",
    "                max_value = attn_weight_cross_part[int(attn_weight_cross_part.size(0)/2):].max(-1)[0].unsqueeze(-1) # 4, 24, 4096, 1\n",
    "                \n",
    "                mask = layout_c.repeat(attn.heads,1,1) # 24, 4096, 333\n",
    "                size_reg = reg_sizes[attn_weight_cross_part.size(2)].repeat(attn.heads,1,1) # 24, 4096, 1\n",
    "                \n",
    "                attn_weight_cross_part += (mask>0)*size_reg*creg*treg*(max_value-attn_weight_cross_part)\n",
    "                attn_weight_cross_part -= ~(mask>0)*size_reg*creg*treg*(attn_weight_cross_part-min_value)\n",
    "                \n",
    "                # modify the cross-attention part in attn_weight\n",
    "                attn_weight[:,:,:image_length,image_length:] = attn_weight_cross_part\n",
    "                \n",
    "                attn_weight += attn_bias\n",
    "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "                attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "        \n",
    "        else:\n",
    "            attn_weight += attn_bias\n",
    "            attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "            attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "        # [batch_size, n_heads, seq_len, head_dim]\n",
    "        hidden_states = attn_weight @ value\n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, hidden_dim]\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # Handle cross-attention output\n",
    "        if encoder_hidden_states is not None:\n",
    "            # Split self-attention and cross-attention outputs\n",
    "            hidden_states, encoder_hidden_states = (\n",
    "                hidden_states[:, : residual.shape[1]],\n",
    "                hidden_states[:, residual.shape[1] :],\n",
    "            )\n",
    "            if not attn.context_pre_only:\n",
    "                encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n",
    "\n",
    "        # Final linear projection and dropout\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        # Return appropriate output based on whether cross-attention was used\n",
    "        if encoder_hidden_states is not None:\n",
    "            return hidden_states, encoder_hidden_states\n",
    "        else:\n",
    "            return hidden_states\n",
    "joint_attn_processor = JointAttnProcessor_mod()\n",
    "def mod_forward_sd3(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    encoder_hidden_states = None,\n",
    "    attention_mask = None,\n",
    "    **cross_attention_kwargs,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    The forward method of the `Attention` class.\n",
    "\n",
    "    Args:\n",
    "        hidden_states (`torch.Tensor`):\n",
    "            The hidden states of the query.\n",
    "        encoder_hidden_states (`torch.Tensor`, *optional*):\n",
    "            The hidden states of the encoder.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention mask to use. If `None`, no mask is applied.\n",
    "        **cross_attention_kwargs:\n",
    "            Additional keyword arguments to pass along to the cross attention.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: The output of the attention layer.\n",
    "    \"\"\"\n",
    "    # The `Attention` class can call different attention processors / attention functions\n",
    "    # here we simply pass along all tensors to the selected processor class\n",
    "    # For standard processors that are defined here, `**cross_attention_kwargs` is empty\n",
    "\n",
    "    attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n",
    "    quiet_attn_parameters = {\"ip_adapter_masks\"}\n",
    "    unused_kwargs = [\n",
    "        k for k, _ in cross_attention_kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters\n",
    "    ]\n",
    "\n",
    "    cross_attention_kwargs = {k: w for k, w in cross_attention_kwargs.items() if k in attn_parameters}\n",
    "    if isinstance(self.processor, AttnProcessor2_0):\n",
    "        pass\n",
    "    else:\n",
    "        self.processor = joint_attn_processor\n",
    "    return self.processor(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        **cross_attention_kwargs,\n",
    "    )\n",
    "for _module in pipe.transformer.modules():\n",
    "    if _module.__class__.__name__ == \"Attention\":\n",
    "        _module.__class__.__call__ = mod_forward_sd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec8f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "###### prep for sreg ###### \n",
    "###########################\n",
    "sreg_maps = {}\n",
    "reg_sizes = {}\n",
    "\n",
    "res = 64\n",
    "layouts_s = F.interpolate(layouts,(res, res),mode='nearest')\n",
    "layouts_s = (layouts_s.view(layouts_s.size(0),1,-1)*layouts_s.view(layouts_s.size(0),-1,1)).sum(0).unsqueeze(0).repeat(1, 1, 1)\n",
    "layouts_s = layouts_s.bool()\n",
    "reg_sizes[np.power(res, 2)] = 1-1.*layouts_s.sum(-1, keepdim=True)/(np.power(res, 2))\n",
    "sreg_maps[np.power(res, 2)] = layouts_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ac752bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4570],\n",
       "         [0.4570],\n",
       "         [0.4570],\n",
       "         ...,\n",
       "         [0.4570],\n",
       "         [0.4570],\n",
       "         [0.4570]]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_sizes[4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851eb739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 4096])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sreg_maps[4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a42ba815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 -th segment is handled.\n",
      "a sink 2 -th segment is handled.\n",
      "A cat is sitting 3 -th segment is handled.\n",
      " 1 -th segment is handled.\n",
      "a sink 2 -th segment is handled.\n",
      "A cat is sitting 3 -th segment is handled.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "###### prep for creg ######\n",
    "###########################\n",
    "pww_maps = torch.zeros(1, 77, sp_sz, sp_sz).to(device)\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input['length'][i] - 2\n",
    "    widx = text_input['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "            cond_embeddings[0][j:j+wlen] = cond_embeddings[i][1:1+wlen]\n",
    "            print(prompts[i], i, '-th segment is handled.')\n",
    "            break\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input_2['length'][i] - 2\n",
    "    widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "            cond_embeddings_2[0][j:j+wlen] = cond_embeddings_2[i][1:1+wlen]\n",
    "            print(prompts[i], i, '-th segment is handled.')\n",
    "            break\n",
    "\n",
    "layout_c_1 = F.interpolate(pww_maps,(32,24),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1) # CLIP model 1 [1, 768, 77]\n",
    "layout_c_2 = F.interpolate(pww_maps,(32,40),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1) # CLIP model 2 [1, 1280, 77]\n",
    "layout_c_3 = torch.zeros(1, 4096, 256).to(device) # T5 model [1, 4096, 256], zero pad as it is not used\n",
    "\n",
    "# concat layout_c_1, layout_c_2, then pad to 4096\n",
    "layout_c = torch.cat([layout_c_1, layout_c_2], dim=1) # [1, 2048, 77]\n",
    "layout_c = torch.nn.functional.pad(layout_c, (0, 0, 0, 4096 - layout_c.shape[1]))\n",
    "# concat layout_c, layout_c_3 to [1, 4096, 333]\n",
    "layout_c = torch.cat([layout_c, layout_c_3], dim=2) # [1, 4096, 333]\n",
    "\n",
    "layout_c = layout_c.bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1c4867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 333])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d2f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# ###### prep for creg ######\n",
    "# ###########################\n",
    "# pww_maps = torch.zeros(1, 77, sp_sz, sp_sz).to(device)\n",
    "# for i in range(1,len(prompts)):\n",
    "#     wlen = text_input['length'][i] - 2\n",
    "#     widx = text_input['input_ids'][i][1:1+wlen]\n",
    "#     for j in range(77):\n",
    "#         if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "#             pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "#             cond_embeddings[0][j:j+wlen] = cond_embeddings[i][1:1+wlen]\n",
    "#             print(prompts[i], i, '-th segment is handled.')\n",
    "#             break\n",
    "\n",
    "# for i in range(1,len(prompts)):\n",
    "#     wlen = text_input_2['length'][i] - 2\n",
    "#     widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "#     for j in range(77):\n",
    "#         if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "#             pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "#             cond_embeddings_2[0][j:j+wlen] = cond_embeddings_2[i][1:1+wlen]\n",
    "#             print(prompts[i], i, '-th segment is handled.')\n",
    "#             break\n",
    "            \n",
    "# creg_maps = {}\n",
    "# res = 64\n",
    "# layout_c = F.interpolate(pww_maps,(res,res),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1)\n",
    "# creg_maps[np.power(res, 2)] = layout_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6cc1ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 128, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pww_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cf9e824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(pww_maps,(res,res),mode='nearest').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9b54bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 4096])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sreg_maps[4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f59a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(sreg_maps[4096][0].cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.title('Sreg Map')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdc85145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:441: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  pww_maps = torch.zeros(1, 77, self.transformer.sample_size, self.transformer.sample_size).to(device)\n",
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:453: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  res = int(self.transformer.sample_size/np.power(2,r))\n",
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:466: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  pww_maps_2 = torch.zeros(1, 77, self.transformer.sample_size, self.transformer.sample_size).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_embed shape torch.Size([4, 77, 768])\n",
      "prompt_2_embed shape torch.Size([4, 77, 1280])\n",
      "clip_prompt_embeds shape torch.Size([4, 77, 2048])\n",
      "t5_prompt_embed shape torch.Size([4, 256, 4096])\n",
      "clip_prompt_embeds shape after padding torch.Size([4, 77, 4096])\n",
      "prompt_embeds shape after concat torch.Size([4, 333, 4096])\n",
      "pooled_prompt_embeds shape torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "reg_part = .3\n",
    "sreg = .3\n",
    "creg = 1.\n",
    "\n",
    "text_con=None\n",
    "COUNT = 0\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompts, layouts=layouts, num_inference_steps=STEPS, guidance_scale=7).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fa600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)+np.asarray(image[0])]))\n",
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+np.asarray(image[0])))\n",
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+[np.asarray(image[i]) for i in range(len(image))], 1))\n",
    "Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+[np.asarray(image[0])], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from attention_map_diffusers import (\n",
    "    attn_maps,\n",
    "    init_pipeline,\n",
    "    save_attention_maps\n",
    ")\n",
    "device = \"cuda\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    cache_dir='./models/diffusers/',\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "##### 1. Replace modules and Register hook #####\n",
    "pipe = init_pipeline(pipe)\n",
    "################################################\n",
    "\n",
    "# recommend not using batch operations for sd3, as cpu memory could be exceeded.\n",
    "prompts = [\n",
    "    # \"A photo of a puppy wearing a hat.\",\n",
    "    \"A capybara holding a sign that reads Hello World.\",\n",
    "]\n",
    "\n",
    "images = pipe(\n",
    "    prompts,\n",
    "    num_inference_steps=5,\n",
    "    guidance_scale=4.5,\n",
    ").images\n",
    "\n",
    "for batch, image in enumerate(images):\n",
    "    image.save(f'{batch}-sd3-5.png')\n",
    "\n",
    "##### 2. Process and Save attention map #####\n",
    "save_attention_maps(attn_maps, pipe.tokenizer, prompts, base_dir='attn_maps-sd3-5', unconditional=True)\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
