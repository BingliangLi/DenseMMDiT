{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366bf9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DenseDiffusion/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import diffusers\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion_3.pipeline_dense_stable_diffusion_3 import DenseStableDiffusion3Pipeline\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "import transformers\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    T5EncoderModel,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "token = \"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\"\n",
    "device= \"cuda\"\n",
    "\n",
    "with open('./dataset/testset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "layout_img_root = './dataset/testset_layout/'\n",
    "# with open('./dataset/valset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)\n",
    "# layout_img_root = './dataset/valset_layout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da96e1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]it/s]\n",
      "Loading pipeline components...:  78%|███████▊  | 7/9 [00:10<00:04,  2.02s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:12<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "pipe = DenseStableDiffusion3Pipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    cache_dir='./models/diffusers/',\n",
    "    # text_encoder_3=None,\n",
    "    # tokenizer_3=None,\n",
    "    torch_dtype=torch.bfloat16\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d79268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9580/754738756.py:4: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  sp_sz = pipe.transformer.sample_size\n"
     ]
    }
   ],
   "source": [
    "STEPS=10\n",
    "pipe.scheduler.set_timesteps(STEPS)\n",
    "timesteps = pipe.scheduler.timesteps\n",
    "sp_sz = pipe.transformer.sample_size\n",
    "bsz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8e92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 17\n",
    "layout_img_path = layout_img_root+str(idx)+'.png'\n",
    "prompts = [dataset[idx]['textual_condition']] + dataset[idx]['segment_descriptions']\n",
    "\n",
    "########### tokenizer 1 and text encoder 1\n",
    "text_input = pipe.tokenizer(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                            max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input = pipe.tokenizer([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input['length'][i] - 2\n",
    "    widx = text_input['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            break\n",
    "\n",
    "############ tokenizer 2 and text encoder 2\n",
    "text_input_2 = pipe.tokenizer_2(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                               max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings_2 = pipe.text_encoder_2(text_input_2.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input_2 = pipe.tokenizer_2([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings_2 = pipe.text_encoder_2(uncond_input_2.input_ids.to(device))[0]\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input_2['length'][i] - 2\n",
    "    widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            break\n",
    "        \n",
    "############ tokenizer 3 and text encoder 3\n",
    "text_input_3 = pipe.tokenizer_3(prompts, padding=\"max_length\", return_length=True, return_overflowing_tokens=False, \n",
    "                               max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "cond_embeddings_3 = pipe.text_encoder_3(text_input_3.input_ids.to(device))[0]\n",
    "\n",
    "uncond_input_3 = pipe.tokenizer_3([\"\"]*bsz, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "                              truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings_3 = pipe.text_encoder_3(uncond_input_3.input_ids.to(device))[0]\n",
    "\n",
    "############\n",
    "layout_img_ = np.asarray(Image.open(layout_img_path).resize([sp_sz*8,sp_sz*8]))[:,:,:3]\n",
    "unique, counts = np.unique(np.reshape(layout_img_,(-1,3)), axis=0, return_counts=True)\n",
    "sorted_idx = np.argsort(-counts)\n",
    "\n",
    "layouts_ = []\n",
    "\n",
    "for i in range(len(prompts)-1):\n",
    "    if (unique[sorted_idx[i]] == [0, 0, 0]).all() or (unique[sorted_idx[i]] == [255, 255, 255]).all():\n",
    "        layouts_ = [((layout_img_ == unique[sorted_idx[i]]).sum(-1)==3).astype(np.uint8)] + layouts_\n",
    "    else:\n",
    "        layouts_.append(((layout_img_ == unique[sorted_idx[i]]).sum(-1)==3).astype(np.uint8))\n",
    "        \n",
    "layouts = [torch.FloatTensor(l).unsqueeze(0).unsqueeze(0).cuda() for l in layouts_]\n",
    "layouts = F.interpolate(torch.cat(layouts),(sp_sz,sp_sz),mode='nearest')\n",
    "\n",
    "layouts_cross = [torch.FloatTensor(l).unsqueeze(0).unsqueeze(0).cuda() for l in layouts_]\n",
    "layouts_cross = F.interpolate(torch.cat(layouts_cross),(sp_sz,sp_sz),mode='nearest')\n",
    "############\n",
    "# print('\\n'.join(prompts))\n",
    "# Image.fromarray(np.concatenate([255*_.squeeze().cpu().numpy() for _ in layouts], 1).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df45a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "from diffusers.models.attention import Attention\n",
    "import math\n",
    "\n",
    "class JointAttnProcessor_mod:\n",
    "    \"\"\"\n",
    "    Attention processor for SD3-like self-attention projections.\n",
    "    This processor handles both self-attention and cross-attention mechanisms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,  # Attention module\n",
    "        hidden_states: torch.FloatTensor,  # Input tensor [batch_size, seq_len, hidden_dim]\n",
    "        encoder_hidden_states: torch.FloatTensor = None,  # Optional context tensor [batch_size, context_len, hidden_dim]\n",
    "        attention_mask = None,  # Optional mask tensor [batch_size, seq_len]\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # Store residual for skip connection\n",
    "        residual = hidden_states  # [batch_size, seq_len, hidden_dim]\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        # Project input into query, key, value spaces\n",
    "        # Each projection: [batch_size, seq_len, hidden_dim]\n",
    "        query = attn.to_q(hidden_states)\n",
    "        key = attn.to_k(hidden_states)\n",
    "        value = attn.to_v(hidden_states)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        inner_dim = key.shape[-1]  # Total dimension across all heads\n",
    "        head_dim = inner_dim // attn.heads  # Dimension per attention head\n",
    "\n",
    "        # Reshape tensors to [batch_size, n_heads, seq_len, head_dim]\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply normalization if specified\n",
    "        if attn.norm_q is not None:\n",
    "            query = attn.norm_q(query)\n",
    "        if attn.norm_k is not None:\n",
    "            key = attn.norm_k(key)\n",
    "        \n",
    "\n",
    "        # Handle cross-attention if encoder_hidden_states is provided\n",
    "        if encoder_hidden_states is not None:\n",
    "            # Project encoder states to query, key, value\n",
    "            # [batch_size, context_len, hidden_dim] -> [batch_size, n_heads, context_len, head_dim]\n",
    "            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)\n",
    "            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n",
    "            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n",
    "\n",
    "            # Reshape encoder projections\n",
    "            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(\n",
    "                batch_size, -1, attn.heads, head_dim\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "            # Apply normalization to encoder projections if specified\n",
    "            if attn.norm_added_q is not None:\n",
    "                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)\n",
    "            if attn.norm_added_k is not None:\n",
    "                encoder_hidden_states_key_proj = attn.norm_added_k(encoder_hidden_states_key_proj)\n",
    "\n",
    "            # Concatenate self and cross attention tensors\n",
    "            # [batch_size, n_heads, seq_len + context_len, head_dim]\n",
    "            query = torch.cat([query, encoder_hidden_states_query_proj], dim=2)\n",
    "            key = torch.cat([key, encoder_hidden_states_key_proj], dim=2)\n",
    "            value = torch.cat([value, encoder_hidden_states_value_proj], dim=2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        L, S = query.size(-2), key.size(-2)  # L: target sequence length, S: source sequence length\n",
    "        scale_factor = 1 / math.sqrt(query.size(-1))  # Scaling factor for numerical stability\n",
    "        attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dtype == torch.bool:\n",
    "                attn_bias.masked_fill_(attention_mask.logical_not(), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_bias += attention_mask\n",
    "\n",
    "        # Compute attention weights and apply them to values\n",
    "        # [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        \n",
    "        ################################################# regulation for self attention and self attention\n",
    "        sa_ = True if encoder_hidden_states is None else False\n",
    "        global COUNT\n",
    "        if COUNT/37 < STEPS * reg_part:\n",
    "            treg = torch.pow(timesteps[COUNT//37]/1000, 5)\n",
    "            if sa_:\n",
    "                # self attention regulation\n",
    "                min_value = attn_weight[int(attn_weight.size(0)/2):].min(-1)[0].unsqueeze(-1)\n",
    "                max_value = attn_weight[int(attn_weight.size(0)/2):].max(-1)[0].unsqueeze(-1)  \n",
    "                # print(\"attn.heads\", attn.heads)\n",
    "                mask = sreg_maps[attn_weight.size(2)].repeat(attn.heads,1,1)\n",
    "                # print(\"mask\", mask.shape)\n",
    "                size_reg = reg_sizes[attn_weight.size(2)].repeat(attn.heads,1,1)\n",
    "                # print(\"size_reg\", size_reg.shape)\n",
    "                \n",
    "                # Apply positive and negative regulation for self-attention\n",
    "                attn_weight[int(attn_weight.size(0)/2):] += (mask>0)*size_reg*sreg*treg*(max_value-attn_weight[int(attn_weight.size(0)/2):])\n",
    "                attn_weight[int(attn_weight.size(0)/2):] -= ~(mask>0)*size_reg*sreg*treg*(attn_weight[int(attn_weight.size(0)/2):]-min_value)\n",
    "                \n",
    "                attn_weight += attn_bias\n",
    "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "                attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "            else:\n",
    "                # cross attention regulation\n",
    "                \n",
    "                # The upper-left and bottom-right parts of the matrix represent self-attention, while the upper-right and bottom-left parts represent cross-attention.\n",
    "                # Here we only apply regulation to the cross-attention part.\n",
    "                image_length = query.shape[2] - encoder_hidden_states_query_proj.shape[2]\n",
    "                attn_weight_cross_part = attn_weight[:,:,:image_length,image_length:] # (4,24,4429,4429) -> (4,24,4096,333)\n",
    "\n",
    "                min_value = attn_weight_cross_part[int(attn_weight_cross_part.size(0)/2):].min(-1)[0].unsqueeze(-1)\n",
    "                max_value = attn_weight_cross_part[int(attn_weight_cross_part.size(0)/2):].max(-1)[0].unsqueeze(-1)\n",
    "                \n",
    "                print(\"layout_c\", layout_c.shape)\n",
    "                mask = layout_c.repeat(attn.heads,1,1)\n",
    "                print(\"cross mask\", mask.shape)\n",
    "                size_reg = reg_sizes[attn_weight_cross_part.size(2)].repeat(attn.heads,1,1)\n",
    "                # print((mask>0)*size_reg*creg*treg.shape)\n",
    "                attn_weight_cross_part += (mask>0)*size_reg*creg*treg*(max_value-attn_weight_cross_part)\n",
    "                attn_weight_cross_part -= ~(mask>0)*size_reg*creg*treg*(attn_weight_cross_part-min_value)\n",
    "                \n",
    "                # modify the cross-attention part in attn_weight\n",
    "                attn_weight[:,:,:image_length,image_length:] = attn_weight_cross_part\n",
    "                \n",
    "                attn_weight += attn_bias\n",
    "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "                attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "        \n",
    "        else:\n",
    "            attn_weight += attn_bias\n",
    "            attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "            attn_weight = torch.dropout(attn_weight, 0.0, train=False)\n",
    "        # [batch_size, n_heads, seq_len, head_dim]\n",
    "        hidden_states = attn_weight @ value\n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, hidden_dim]\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # Handle cross-attention output\n",
    "        if encoder_hidden_states is not None:\n",
    "            # Split self-attention and cross-attention outputs\n",
    "            hidden_states, encoder_hidden_states = (\n",
    "                hidden_states[:, : residual.shape[1]],\n",
    "                hidden_states[:, residual.shape[1] :],\n",
    "            )\n",
    "            if not attn.context_pre_only:\n",
    "                encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n",
    "\n",
    "        # Final linear projection and dropout\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        # Return appropriate output based on whether cross-attention was used\n",
    "        if encoder_hidden_states is not None:\n",
    "            return hidden_states, encoder_hidden_states\n",
    "        else:\n",
    "            return hidden_states\n",
    "joint_attn_processor = JointAttnProcessor_mod()\n",
    "def mod_forward_sd3(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    encoder_hidden_states = None,\n",
    "    attention_mask = None,\n",
    "    **cross_attention_kwargs,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    The forward method of the `Attention` class.\n",
    "\n",
    "    Args:\n",
    "        hidden_states (`torch.Tensor`):\n",
    "            The hidden states of the query.\n",
    "        encoder_hidden_states (`torch.Tensor`, *optional*):\n",
    "            The hidden states of the encoder.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention mask to use. If `None`, no mask is applied.\n",
    "        **cross_attention_kwargs:\n",
    "            Additional keyword arguments to pass along to the cross attention.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: The output of the attention layer.\n",
    "    \"\"\"\n",
    "    # The `Attention` class can call different attention processors / attention functions\n",
    "    # here we simply pass along all tensors to the selected processor class\n",
    "    # For standard processors that are defined here, `**cross_attention_kwargs` is empty\n",
    "\n",
    "    attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n",
    "    quiet_attn_parameters = {\"ip_adapter_masks\"}\n",
    "    unused_kwargs = [\n",
    "        k for k, _ in cross_attention_kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters\n",
    "    ]\n",
    "\n",
    "    cross_attention_kwargs = {k: w for k, w in cross_attention_kwargs.items() if k in attn_parameters}\n",
    "    if isinstance(self.processor, AttnProcessor2_0):\n",
    "        pass\n",
    "    else:\n",
    "        self.processor = joint_attn_processor\n",
    "    return self.processor(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        **cross_attention_kwargs,\n",
    "    )\n",
    "for _module in pipe.transformer.modules():\n",
    "    if _module.__class__.__name__ == \"Attention\":\n",
    "        _module.__class__.__call__ = mod_forward_sd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec8f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "###### prep for sreg ###### \n",
    "###########################\n",
    "sreg_maps = {}\n",
    "reg_sizes = {}\n",
    "\n",
    "res = 64\n",
    "layouts_s = F.interpolate(layouts,(res, res),mode='nearest')\n",
    "layouts_s = (layouts_s.view(layouts_s.size(0),1,-1)*layouts_s.view(layouts_s.size(0),-1,1)).sum(0).unsqueeze(0).repeat(1, 1, 1)\n",
    "layouts_s = layouts_s.bool()\n",
    "reg_sizes[np.power(res, 2)] = 1-1.*layouts_s.sum(-1, keepdim=True)/(np.power(res, 2))\n",
    "sreg_maps[np.power(res, 2)] = layouts_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851eb739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sreg_maps[4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42ba815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 -th segment is handled.\n",
      "a sink 2 -th segment is handled.\n",
      "A cat is sitting 3 -th segment is handled.\n",
      " 1 -th segment is handled.\n",
      "a sink 2 -th segment is handled.\n",
      "A cat is sitting 3 -th segment is handled.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "###### prep for creg ######\n",
    "###########################\n",
    "pww_maps = torch.zeros(1, 77, sp_sz, sp_sz).to(device)\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input['length'][i] - 2\n",
    "    widx = text_input['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "            cond_embeddings[0][j:j+wlen] = cond_embeddings[i][1:1+wlen]\n",
    "            print(prompts[i], i, '-th segment is handled.')\n",
    "            break\n",
    "\n",
    "for i in range(1,len(prompts)):\n",
    "    wlen = text_input_2['length'][i] - 2\n",
    "    widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "    for j in range(77):\n",
    "        if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "            pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "            cond_embeddings_2[0][j:j+wlen] = cond_embeddings_2[i][1:1+wlen]\n",
    "            print(prompts[i], i, '-th segment is handled.')\n",
    "            break\n",
    "\n",
    "layout_c_1 = F.interpolate(pww_maps,(32,24),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1) # CLIP model 1 [1, 768, 77]\n",
    "layout_c_2 = F.interpolate(pww_maps,(32,40),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1) # CLIP model 2 [1, 1280, 77]\n",
    "layout_c_3 = torch.zeros(1, 4096, 256).to(device) # T5 model [1, 4096, 256], zero pad as it is not used\n",
    "\n",
    "# concat layout_c_1, layout_c_2, then pad to 4096\n",
    "layout_c = torch.cat([layout_c_1, layout_c_2], dim=1) # [1, 2048, 77]\n",
    "layout_c = torch.nn.functional.pad(layout_c, (0, 0, 0, 4096 - layout_c.shape[1]))\n",
    "# concat layout_c, layout_c_3 to [1, 4096, 333]\n",
    "layout_c = torch.cat([layout_c, layout_c_3], dim=2) # [1, 4096, 333]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1c4867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 333])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d2f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# ###### prep for creg ######\n",
    "# ###########################\n",
    "# pww_maps = torch.zeros(1, 77, sp_sz, sp_sz).to(device)\n",
    "# for i in range(1,len(prompts)):\n",
    "#     wlen = text_input['length'][i] - 2\n",
    "#     widx = text_input['input_ids'][i][1:1+wlen]\n",
    "#     for j in range(77):\n",
    "#         if (text_input['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "#             pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "#             cond_embeddings[0][j:j+wlen] = cond_embeddings[i][1:1+wlen]\n",
    "#             print(prompts[i], i, '-th segment is handled.')\n",
    "#             break\n",
    "\n",
    "# for i in range(1,len(prompts)):\n",
    "#     wlen = text_input_2['length'][i] - 2\n",
    "#     widx = text_input_2['input_ids'][i][1:1+wlen]\n",
    "#     for j in range(77):\n",
    "#         if (text_input_2['input_ids'][0][j:j+wlen] == widx).sum() == wlen:\n",
    "#             pww_maps[:,j:j+wlen,:,:] = layouts[i-1:i]\n",
    "#             cond_embeddings_2[0][j:j+wlen] = cond_embeddings_2[i][1:1+wlen]\n",
    "#             print(prompts[i], i, '-th segment is handled.')\n",
    "#             break\n",
    "            \n",
    "# creg_maps = {}\n",
    "# res = 64\n",
    "# layout_c = F.interpolate(pww_maps,(res,res),mode='nearest').view(1,77,-1).permute(0,2,1).repeat(bsz,1,1)\n",
    "# creg_maps[np.power(res, 2)] = layout_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cc1ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 128, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pww_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf9e824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(pww_maps,(res,res),mode='nearest').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b54bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sreg_maps[4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f59a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(sreg_maps[4096][0].cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.title('Sreg Map')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc85145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:441: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  pww_maps = torch.zeros(1, 77, self.transformer.sample_size, self.transformer.sample_size).to(device)\n",
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:453: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  res = int(self.transformer.sample_size/np.power(2,r))\n",
      "/home/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:466: FutureWarning: Accessing config attribute `sample_size` directly via 'SD3Transformer2DModel' object attribute is deprecated. Please access 'sample_size' over 'SD3Transformer2DModel's config object instead, e.g. 'unet.config.sample_size'.\n",
      "  pww_maps_2 = torch.zeros(1, 77, self.transformer.sample_size, self.transformer.sample_size).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_embed shape torch.Size([4, 77, 768])\n",
      "prompt_2_embed shape torch.Size([4, 77, 1280])\n",
      "clip_prompt_embeds shape torch.Size([4, 77, 2048])\n",
      "t5_prompt_embed shape torch.Size([4, 256, 4096])\n",
      "clip_prompt_embeds shape after padding torch.Size([4, 77, 4096])\n",
      "prompt_embeds shape after concat torch.Size([4, 333, 4096])\n",
      "pooled_prompt_embeds shape torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layout_c torch.Size([1, 4096, 333])\n",
      "cross mask torch.Size([24, 4096, 333])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/DenseDiffusion/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DenseDiffusion/diffusers/src/diffusers/pipelines/stable_diffusion_3/pipeline_dense_stable_diffusion_3.py:1002\u001b[0m, in \u001b[0;36mDenseStableDiffusion3Pipeline.__call__\u001b[0;34m(self, prompt, prompt_2, prompt_3, height, width, num_inference_steps, sigmas, guidance_scale, negative_prompt, negative_prompt_2, negative_prompt_3, num_images_per_prompt, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length, skip_guidance_layers, skip_layer_guidance_scale, skip_layer_guidance_stop, skip_layer_guidance_start, mu, layouts)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m timestep \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mexpand(latent_model_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1002\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m~/DenseDiffusion/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DenseDiffusion/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DenseDiffusion/diffusers/src/diffusers/models/transformers/transformer_sd3.py:419\u001b[0m, in \u001b[0;36mSD3Transformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, block_controlnet_hidden_states, joint_attention_kwargs, return_dict, skip_layers)\u001b[0m\n\u001b[1;32m    409\u001b[0m     encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    410\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    411\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_skip:\n\u001b[0;32m--> 419\u001b[0m     encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# controlnet residual\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_controlnet_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m block\u001b[38;5;241m.\u001b[39mcontext_pre_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/DenseDiffusion/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DenseDiffusion/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DenseDiffusion/diffusers/src/diffusers/models/attention.py:213\u001b[0m, in \u001b[0;36mJointTransformerBlock.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, temb, joint_attention_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1_context(\n\u001b[1;32m    209\u001b[0m         encoder_hidden_states, emb\u001b[38;5;241m=\u001b[39mtemb\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Attention.\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m attn_output, context_attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_encoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Process attention outputs for the `hidden_states`.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "Cell \u001b[0;32mIn[5], line 214\u001b[0m, in \u001b[0;36mmod_forward_sd3\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m joint_attn_processor\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 135\u001b[0m, in \u001b[0;36mJointAttnProcessor_mod.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m size_reg \u001b[38;5;241m=\u001b[39m reg_sizes[attn_weight_cross_part\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)]\u001b[38;5;241m.\u001b[39mrepeat(attn\u001b[38;5;241m.\u001b[39mheads,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# print((mask>0)*size_reg*creg*treg.shape)\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m attn_weight_cross_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (mask\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m*\u001b[39msize_reg\u001b[38;5;241m*\u001b[39mcreg\u001b[38;5;241m*\u001b[39mtreg\u001b[38;5;241m*\u001b[39m(\u001b[43mmax_value\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mattn_weight_cross_part\u001b[49m)\n\u001b[1;32m    136\u001b[0m attn_weight_cross_part \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(mask\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m*\u001b[39msize_reg\u001b[38;5;241m*\u001b[39mcreg\u001b[38;5;241m*\u001b[39mtreg\u001b[38;5;241m*\u001b[39m(attn_weight_cross_part\u001b[38;5;241m-\u001b[39mmin_value)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# modify the cross-attention part in attn_weight\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "reg_part = .3\n",
    "sreg = .3\n",
    "creg = 1.\n",
    "\n",
    "text_con=None\n",
    "COUNT = 0\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompts, layouts=layouts, num_inference_steps=STEPS, guidance_scale=7).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fa600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)+np.asarray(image[0])]))\n",
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+np.asarray(image[0])))\n",
    "# Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+[np.asarray(image[i]) for i in range(len(image))], 1))\n",
    "Image.fromarray(np.concatenate([layout_img_.astype(np.uint8)]+[np.asarray(image[0])], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from attention_map_diffusers import (\n",
    "    attn_maps,\n",
    "    init_pipeline,\n",
    "    save_attention_maps\n",
    ")\n",
    "device = \"cuda\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    cache_dir='./models/diffusers/',\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "##### 1. Replace modules and Register hook #####\n",
    "pipe = init_pipeline(pipe)\n",
    "################################################\n",
    "\n",
    "# recommend not using batch operations for sd3, as cpu memory could be exceeded.\n",
    "prompts = [\n",
    "    # \"A photo of a puppy wearing a hat.\",\n",
    "    \"A capybara holding a sign that reads Hello World.\",\n",
    "]\n",
    "\n",
    "images = pipe(\n",
    "    prompts,\n",
    "    num_inference_steps=5,\n",
    "    guidance_scale=4.5,\n",
    ").images\n",
    "\n",
    "for batch, image in enumerate(images):\n",
    "    image.save(f'{batch}-sd3-5.png')\n",
    "\n",
    "##### 2. Process and Save attention map #####\n",
    "save_attention_maps(attn_maps, pipe.tokenizer, prompts, base_dir='attn_maps-sd3-5', unconditional=True)\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
